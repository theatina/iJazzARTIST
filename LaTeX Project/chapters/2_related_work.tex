\chapter{RELATED WORK} \label{chapter:related_work}
%Music generation is a broad subject and has been examined for %years, with researchers using multiple theoretical approaches and %methods for this task. 

    \section{Algorithmic Composition} 

    \subsection{Illiac} \label{subsec:illiac}
    Computer generated composition started with Lejaren Hiller and Leonard Isaacson at the University of Illinois in 1955, who used the Illiac high-speed digital computer to program basic material and stylistic parameters, which resulted in the Illiac Suite (1957). First, the score of the piece was composed and then it was transposed into musical notation in order to be read and performed by a string quartet. The process that Hiller and Isaacson have followed and resulted in the Illiac Suite, was to first generate certain ``raw materials" using the computer, then modify these materials according to multiple functions, and finally distinguish and select the best results from the modified musical materials according to various rules \cite{Alpern1995TechniquesFA}. 

    \subsection{MUSICOMP} \label{subsec:musicomp}
    The generator - modifier - selector paradigm explained in section \ref{subsec:illiac}, was also applied to MUSICOMP ( MUsic SImulator - Interpreter for COMpositional Procedures ), one of the first computer software developed for the purpose of simulating musical composition procedures, programmed by Lejaren Hiller and Robert Baker in Fortran and Scatre. Between the late 1950s and early 1960s, MUSICOMP composed the ``Computer Cantata". MUSICOMP was written as a library of subroutines, making the process of writing composition programs much easier, as the programmer-composer had the ability to use the routines within a larger program that suited his or her own style \cite{Alpern1995TechniquesFA}. The idea of building compact, well-defined compositional functions and then assembling them together, would prove efficient and provide the system a degree of flexibility and generality \cite{Alpern1995TechniquesFA}, which has made the aforementioned approach a popular one in many algorithmic composition systems, even to this day.


    
    \subsection{UPIC}
    UPIC (Unité Polyagogique Informatique CEMAMu) is a computerised musical composition tool, devised by the well-known composer Iannis Xenakis. UPIC's development was completed at the Centre d'Etudes de Mathématique et Automatique Musicales (CEMAMu) in Paris, 1977. Xenakis used it on his subsequent piece Mycènes Alpha (1978), and it has been used by several composers such as Julio Estrada, Jean-Claude Risset , François-Bernard Mâche, Takehito Shimazu, Mari King and Curtis Roads.

    Physically, the UPIC is a digitising tablet linked to a computer, which has a vector display. The user draws waveforms and volume envelopes on the tablet, which are showed on the computer. After the waveforms have been stored, the user can deploy them to create compositions, by drawing on the tablet, using the X-axis to represent time, and the Y-axis to represent pitch. The compositions can be stretched in duration from a few seconds to an hour. The compositions can be transposed, reversed, inverted and subjected to a number of algorithmic transformations. 

    \subsection{IBM 7090}
    Another pioneering application of the computer in algorithmic composition is that of Iannis Xenakis, who used the IBM 7090 computer's high-speed computations to perform calculations of various probability theories to aid in several compositions (such as Atrées (1962) and Morsima-Amorsima (1962) ). The program he created, would produce a score, making use of a list of note densities and probabilistic weights inserted by the programmer, specifying the duration and density of ``sound events", leaving the parameters of pitch, velocity, and dynamics to a random number generator. 


    \section{Non - Adaptive Generative Systems}
    
        \subsection{Stochastic Methods} \label{subsec:stochastic_methods}
        ``Stochastic" is a term from mathematics which designates such a process, ``in which a sequence of values is drawn from a corresponding sequence of jointly distributed random variables" (Webster's dictionary).
        
        ``Stochastic music" is based on a process in which the probabilities of proceeding from one state, or set of states, is [sic] defined \cite{audibleDesign}. The temporal evolution of the process is therefore governed by a kind of weighted randomness, which can be chosen to give everything from an entirely determined outcome to an entirely unpredictable one. In the stochastic method, a great number of the creative decisions concerning attributes and parameters, are merely left to chance.

        Such approaches, are characterised by randomness and can perform simple tasks such as generate a random series of notes, as in Mozart's Dice Music, while at the same time, a great amount of conceptual complexity can be introduced to the computations through the computer with statistical theory and Markov chains. Another example of non-computer-oriented ``stochastic" composition can be found in Karlheinz Stockhausen's Klaveirstucke XI. In this work, various music fragments are to be performed by a pianist in a random sequence. Additionally, Stochos \cite{Stochos} as described by the authors, is a real-time event generator implemented as a patch, allowing the assignment of stochastic, chaotic and deterministic curves to different sound transformation and synthesis parameters, through a control interface.

    
        \subsection{Rule - Based Approaches} \label{subsec:rule_based}

        A second approach to algorithmic composition using the computer is that of ``rule-based" systems and formal grammars. Musicologists have devised several abstract rules for defining musical style and for expressing what is ``allowed” in specific types of compositions \cite{maxim_kal_pap_book}. A rule-based process would center around those predefined rules, through which the program progresses. These steps are usually constructed in such a way that the product of the steps leads to the next new step \cite{burns1997algorithmic}. Rather than relying on randomness to make decisions as in the stochastic methods described in section \ref{subsec:stochastic_methods} above, rule-based systems pre-compose a ``grammar", by which the compositional process should behave, once set into motion.  This grammar, designates the formal system of principles by which the possible sentences of a language are generated \cite{burns1997algorithmic}, with, in this case, sentences being musical phrases. Like Hiller's MUSICOMP (\ref{subsec:musicomp}), the generative process usually takes the form of a computer program or a system comprising subroutines, often involving rules stored in databases, that were previously collected or newly invented. Examples of such approaches are the rule-based expert system for harmonization and Schenkerian analysis of chorales in the style of J.S. Bach, the CHORAL, presented by Kemal Ebcioğlu in 1988 \cite{choral_bach} and the approach of Phon-Amnuaisuk et al. \cite{phon2006chorale, Phon_bach_2002}, who introduced a Bach chorale style music knowledge representation, through expert-designed rules, enabling the user to control the developed system's harmonization behaviour.


        Non adaptive generative AI approaches, are based on proper mappings from processed data to musical surfaces for music generation. Cellular Automata, L-systems and non-adaptive/autonomous Swarm Intelligence, described in the following sub-sections are examples of such methods. The musical rules used by the aforementioned systems, are parts of this mapping, for instance, the output data of multiple generative algorithms is numerical, and each possible value produced is mapped to a musical object explicitly. This musical entity can be linked to pitch and/or rhythmic and/or intensity value and/or information related with structure. What renders the output musically meaningful is the design and development of that mapping. Additionally, coming up with proper mappings plays an important role in the system's creativity \cite{maxim_kal_pap_book}.


        \subsubsection{Cellular Automata} \label{subsub:ca}
        Cellular Automata (CA) are normally implemented on a computer as a regular array or matrix of variables, or cells, which normally can have one, two or three dimensions. Each cell may assume values from a finite set of integers and each value is normally associated with a colour. The functioning of such an automaton is monitored on the computer screen as a sequence of changing patterns of tiny coloured cells, according to the tick of an imaginary clock, like an animated film. At each tick of the clock, the values of all cells change simultaneously, according to a set of transition rules that takes into account the values of their neighbourhood \cite{miranda2001evolving}.   
        There have been several approaches at applying CA in the production of electronic music and sonic art, in the fields of overall structural composition, MIDI sequencing and sound synthesis/modification techniques to name a few. Reviews of such mappings can be found in Burraston and Edmonds \cite{burraston2005cellular}, Burraston and Martin \cite{burraston2006digital}, Miranda and Al Biles \cite{miranda2007evolutionary}. Other attempted approaches include symbolic music mapping from CA to notes (piece ``Horos'' by Iannis Xenakis \cite{solomos2005cellular}), similar mappings from CA to MIDI notes \cite{millen2004interactive}, audio synthesizing through granular synthesis, where the location of each active unit indicated which granule of sound would be active at any given time \cite{miranda2001evolving}, or from given spectrograms, where each unit was acting as an amplitude envelope for the respective frequency area of an input spectrogram \cite{serquera2010evolutionary}, as mentioned in Kaliakatsos-Papakostas \cite{maxim_kal_pap_book}.

        \subsubsection{Lindenmayer Systems}
        Lindenmayer Systems (L-Systems) were conceived as a mathematical theory of plant development by Lindenmayer in 1968 \cite{lindenmayer1968mathematical}. The original emphasis was on a mathematical model of cell development and plant topology, that is the neighborhood relations between cells or larger plant modules \cite{prusinkiewicz2012algorithmic}. The central concept of L-systems is that of rewriting, a technique for defining complex objects by successively replacing parts of a simple initial object using a set of rewriting rules. The typical and simplest form of L-systems belongs to the category of deterministic context free grammars(DOL-systems). As a form of formal presentation, the L-systems incorporate an alphabet $V$ of all possible symbols, a set of rules P that associates symbols in the alphabet with a string. Starting from an initial sequence of symbols $\ \omega \in V\textsuperscript{+2}$, denoted as $x\textsubscript{0}$, the rules are applied for each symbol in $x\textsubscript{0}$ resulting in a new sequence of symbols, denoted as $x\textsubscript{1}$. Recursively, the sequence $x\textsubscript{x+1}$ is formed by applying the rules in $P$ on each symbol in $x\textsubscript{n}$. After a number of k steps, the sequence $x\textsubscript{k}$ will constitute a symbol sequence that potentially exhibits interesting structural characteristics on many levels, i.e. not only neighbouring but also remote symbols. L-Systems, similarly to CA (section \ref{subsub:ca}), with their high-level structures are able to provide a sense of structural hierarchy when properly mapped to sound/music \cite{maxim_kal_pap_book}. Prusinkiewicz \cite{prusinkiewicz1986graphical} in his study, which was the first one to transform L-Systems to music, has directly interpreted the generated symbols to notes. McCormack \cite{mccormack1996grammar} has incorporated probabilities about rules associated with the symbols, creating mappings from symbols onto melodic notes. Other applications of L-Systems to music related research, cover sound generation \cite{manousakis2006musical}, the approach of Kaliakatsos-Papakostas et al. \cite{kaliakatsos2012intelligent}, where the produced strings at each next step were truncated to a fixed length, producing strings that had quasi-periodic characteristics at different level, proposing a variation of the L-systems, namely the Finite L-Systems (FL-systems), as well as the exploration of grammatical evolution of the musical FL-Systems' rules \cite{kaliakatsos2012genetic, maxim_kal_pap_book}.


        \subsubsection{Swarm Intelligence}

        Swarm Intelligence (SI) leads to the emergence of collective spatial behavior through the individual readjustment of the location of unique individuals, based on the application of simple rules that update the velocity of each agent according to the location and velocity of other neighboring agents \cite{maxim_kal_pap_book}. Reynolds \cite{reynolds1987flocks} introduced an algorithm in 1987, simulating swarm, herd and flock motion. This algorithm defines the motion of agent based on three components: shoaling, where each agent moves towards the center of mass of its neighboring agents, collision avoidance, where each agent moves away from the agents that are too close and schooling, where the velocity of each agent gets aligned with the mean velocity of the neighboring agents \cite{maxim_kal_pap_book}. Some of the aforementioned characteristics were incorporated to music and sound related interactive agents, developing multiple and wide-ranging approaches. SWARMUSIC \cite{blackwell2002improvised}, is an interactive music improviser, where a particle swarm algorithm generates musical material by a mapping of particle positions onto events in MIDI space, which was the first use of SI in a computer music application. Symbolic music composition has also been performed by such agents \cite{blackwell2003swarm}. Additive synthesis \cite{apergisSonoids2018}, granular synthesis \cite{blackwell2004swarm,blackwell2008swarm}  and granular synthesis with spatial characteristics \cite{wilson2008spatial}, are some more examples of SI applications. Finally, the sonification of the SI agents behavior has been integrated into the “Swarmlake” game \cite{kaliakatsos2014swarm}, which expanded the social behavior with user-controlled commands and attributed different agents with different sound properties, according to specific conditions of the game \cite{maxim_kal_pap_book}.


    \section{Adaptive Generative Systems}
    The multi-level nature of abstraction in musical works, makes even the most complex set of rules impotent to capture deeper patterns and structures, which necessitated the emergence of adaptive models in the field.
    Those kinds of models, have the ability to perform feature learning, explicitly or implicitly. After being trained on a dataset, they can capture the statistical behaviour of musical features either defined by the human agent(explicit methods) or extracted, abstract representations of them into feature latent spaces(implicit methods).

        \subsection{Explicit Learning}
        A music dataset may include explicitly defined features extracted from scores. Probabilistic generative models, are able to capture conditional and combined probabilities of such features; note occurrences, note transitions and conditional probabilities of chords over given notes, to name a few. After collecting statistical information about the dataset, multiple models can be trained, aiming at producing music that reflects characteristics of a given style. Examples of test cases are the four-part harmonisation (composition of a Soprano, Alto, Tenor and Bass (SATB) voice layout, combining those voices properly to form concise harmonic and melodic streams) \cite{allan2005harmonising,suzuki2013four} and the melodic harmonisation (composition of concise harmony over a given melody, without necessarily implementing compositions with specific voicing layout) \cite{simon2008mysong,raczynski2013melody}. Hidden Markov Models (HMMs) \cite{allan2005harmonising, simon2008mysong,raczynski2013melody} and more generalised Bayesian Networks (BNs) \cite{suzuki2013four} are able to perform tasks such as music modeling and generation, based on conditional probabilities across various elements of the dataset (\textit{e.g.} the probability of the current chord given the current observed note sequence).  \cite{maxim_kal_pap_book}. An interesting application of HMMs, MySong \cite{simon2008mysong}, produces chord sequences on the melody sung by the user, after extracting the melodic note fundamentals, with digital signal processing (DSP). The need to capture long-term structure was covered by deploying a recursive hierarchical generalization of the widely used hidden Markov models, the Hierarchical Hidden Markov Models (HHMM) \cite{thornton2009hierarchical}, which also preserve the generalisation capabilities of lower-order Markov models. In addition to the aforementioned approaches, graphical models able to capture long-term structure have also been proposed for modeling chord progression \cite{paiement2005probabilistic} as well as melodic harmonization \cite{paiement2006probabilistic}. Those models, use tree-like nodes that model conditional probabilities from top to bottom. 
        
              
        \subsection{Implicit Learning}
        
        \subsubsection{Machine Learning}
        Features can also be computed implicitly, without a transparent definition. Artificial Neural Networks (ANNs) as described further in section \ref{sec:nn}, are computational models, consisting of interconnected sets of artificial neurons. Typically, neurons are organized in recurrent networks with several interconnected layers. ANNs are used as a machine learning method and in the case of music, they process information from the musical surface and produce more abstract, non-transparent representations in each layer, with no distinction on which aspects of the musical surface are represented at each computational unit of the ANN. Implicit learning with deep ANNs offers important possibilities for categorization and prediction, without still giving clear information about what aspects of the data are more important for taking decisions. The recurrent or looped connections allow such networks to learn local dynamics of data and hold useful information across inputs resulting in developing a local memory. Recurrent neural networks (RNNs) are a class NNs that make use of sequential information in the dataset.
        Since music consists of sequences of musical elements(notes, chords, etc.), RNNs can be successfully employed in various musical applications as they store the data of the previously attended state. The most common are considered to be Long Short-Term Memory (LSTM) cells, described in detail in section \ref{subsec:lstm} and Gated Recurrent Units (GRU). Both of these architectures have multiplicative gates that protect their internal memory from being overwritten too easily, allowing them to handle longer sequences of data. To learn even longer-term structure, attention can be used. Attention is a later advancement, allowing models to access previous information without having to store it in the RNN cell’s state. In attention based LTSMs for example, certain components of the input at random moments can be ``attended" and utilized in order to produce portions of the network's output as opposed to simply output the final calculation of the last LSTM layer.
        

        \subsubsection{Approaches}
        The ANNs were first used during the 1970s and 1980s to analyze musical compositions, creating artificial models of cognitive theories of music \cite{todd1991music} but they were later adapted for music composition \cite{aiMethods}. Todd, who was the first to implement such networks in 1989, used a three-layered RNN that reproduces music in a specific style, but also allows the network to switch styles \cite{todd1989connectionist}. Given a set of one or more composition examples, the ANN was trained to associate a single input configuration to the output temporal sequence of the corresponding composition. Then, feeding input configurations different to the ones used during the training, created melodies interpolated between the ones used during the training. That means for example that if just one melody was used during the training, the result would be an extrapolation from it \cite{aiMethods}. Within the scope of melody generation, Mozer and Soukup examined the effect of psychoacoustical modeling of the inputs \cite{mozer1991connectionist} in 1991. In this method, the inputs and outputs were represented by a vector of coordinates related to pitch, the position(coordinates) on the chromatic circle as well as the position on the circle of fifths and the system was able to generate melodies in the style of Bach chorales \cite{maxim_kal_pap_book}. 

        To improve the aforementioned approach, Mozer proposed the first music generative system to use the recurrent networks technology (RNN's) in 1994, CONCERT (CONnectionist Composer of ERudite Tunes) \cite{mozer1994neural}, which was able to capture long-term structure of the learned melodies. CONCERT underwent training that involved a set of music data, from which the stylistic regularities were extracted, in order to compose new pieces. Significant improvements concerning long term structure was exhibited by deploying the Long Short-Term Memory ( LSTM ) networks, which are analysed in section \ref{subsec:lstm}) below. In Eck and Schmidhuber's approach \cite{eck2002learning}, the LSTM networks, include trainable gates that selectively forget information from the past or recall information from arbitrarily back in time, for generating blues melodies over a given blues chord progression. 

        Another approach for melody generation using LSTM RNNs was developed by Sturm et al. \cite{sturm2015folk}, where folk tunes (monophonic melodies) were modelled in the ABC format, a text and character-based representation that includes metadata, overview of musical setup ($e.g.$ tempo and time signature), metric information ($e.g.$ measure boundaries) and the music surface \cite{maxim_kal_pap_book}. Liang et al. presented a “quasi-monophonic” approach to modeling polyphonic data, where note symbols, bar limits and fermata(a symbol of musical notation indicating that the note should be prolonged beyond the normal duration its note value would indicate) symbols are learned and generated sequentially, from top to bottom and from left to right. In this approach, the ANN was fed with a sequence of single notes, where simultaneous notes simply had the same onset (beginning) time \cite{liang2017automatic}. 

        A similar representation approach with more refined information about the duration and offset time of notes was proposed by Colombo et al. \cite{colombo2018learning}, in which the ANN was fed with a sequence of single notes, where simultaneous notes simply had the same onset (beginning) time. Both of the aforementioned studies incorporated learning and generating polyphonic music in the style of Bach chorales, using LSTM units and Gated Recurrent Units ( GRUs ) respectively. In 2017, Hadjeres et al. introduced DeepBach \cite{hadjeres2017deepbach}, a graphical model based on RNNs, trained to generate four-part chorales in the style of J.S. Bach, which was made steerable by imposing constraints such as notes, rhythms or cadences in the generated score. Those constraints, allow the networks representing each voice(strand, melody or harmony of music within a larger ensemble or a polyphonic musical composition) to have a more robust understanding about the overall metric structure and the activity in each voice \cite{maxim_kal_pap_book}. The approach of Makris et al. for drums rhythm generation was similar to the approach of Hadjeres et al. \cite{hadjeres2017deepbach} in relation to constraints imposition. Indications were given that proper representation of the metric constraints could allow the network to compose rhythms in time signatures that were not encountered during training \cite{maxim_kal_pap_book}. 

        Chord progressions have been taken into account in more recent approaches. Choi et al. introduced text-based LSTM (Long Short-Term Memory) networks for automatic music composition in 2017, designed to learn relationships within text documents that represent chord progressions and drum tracks \cite{choi2016text}. Chu et al. presented a hierarchical multi-layer RNN, where the bottom layers generate the melody and the higher levels produce the drums and chords. Some researchers have also created hybrid systems, which means they combined ANNs with other methods. For example HARMONET \cite{hild1992harmonet}, is a model designed to solve the complex task of four-part choral harmonization in Bach’s style, an architecture comprising three-layers; a feed-forward ANN with a sophisticated encoding of musical information used to extract harmonization information, a rule-based constraint satisfaction algorithm for the chord generation and another ANN designed to add quaver ornaments to the previously generated chords \cite{aiMethods}. NETNEG \cite{lehmann1996netneg} was another hybrid system that combined NNs with rule based agents, an implementation aiming to create compositions of polyphonic music in real time. 
        
        Probabilistic methods can also be deployed in conjunction with ANNs. For example, the work of Verbeurgt et al. in 2004 , utilizes pattern recognition to extract patterns from musical training sequences, constructs a Markov chain the states of which are mapped to each pattern previously extracted and then deploys a neural network to learn which shifts of pitch and duration are allowed for each pattern in the training sequences \cite{verbeurgt2004hybrid}. 


    \subsection{Jazz Music Related Work}
    Jazz music has been in the centre of attention of many MIR research approaches. In 2017, Johnson et al. described a network trained to learn jazz melodies' musical structure over chord progressions, able to create improvised melodies, by calculating and assigning probabilities to prospective musical notes \cite{johnson2017learning}. Trieu and Keller presented JazzGAN in 2018, a generative adversarial network (GAN) using RNNs, for monophonic jazz improvisation over chord progressions, while at the same time introduced Mode Collapse, Creativity, and Chord Harmony metrics aiming for a better understanding and analysis of the musical quality of the generated sequences  \cite{trieu2018jazzgan}. The combination of music theory grammar with the LSTM architecture for jazz music generation, was another approach, presented by Wang et al. in 2019 \cite{wang2019jazz}. The input data of the LSTM model consists of interval, duration, and note category information extracted from MIDI files while the output of the model is a sequence of notes generated according to the transition probability, which is parsed according to the music grammar as the final step of the process. LSTM networks have been broadly used in research subjects involving jazz music composition, and more specifically music generation following the performer's style. 

    De Prisco et al. presented an approach consisting of a One-Class Support Vector Machine for learning the style of a jazz performer, a music splicing system for the composition of the melody following the previously learned style and a LSTM network to predict patterns constrained by the learned style and also guide the splicing system during the composition stage. Hori et al. \cite{hori2017jazz}, developed a complex automatic jazz accompaniment system, based on the stochastic state transition model which approximates a performance trajectory model. A LSTM network and a Deep Belief Network (class of deep neural network consisting of multiple layers, with connections between the layers but not between units within each layer) were also used,in order for the system to ``learn the correlation between the performance of the different jazz piano trio instruments from the performance MIDI data of jazz songs".
    
    
    \subsection{Real Time Applications}
    Real time scenarios however, are rare, with only a few approaches experimenting with or building such systems. Hidaka et al. introduced an automatic accompaniment system for jazz, able to capture the soloist’s intentions during the improvisation process and then output expressive accompaniment of a bass guitar and drums reacting to solo \cite{hidaka1995automatic}. Kaliakatsos-Papakostas et al. proposed a constraint-free improvisation environment, where the most important musical characteristics are automatically adapted to the human performer's playing style, without any prior information  \cite{kaliakatsos2012intelligent}. Differential Evolution (DE), Genetic Algorithms, FL-systems and statistical distribution modeling were employed in order to create the adaptive system, which provides automatic music accompaniment in real-time. Pachet has also proposed a system in 2010, the Continuator, which was able to learn and generate music in any style, either in standalone mode, as continuations of musician’s input, or as interactive improvisation back up in real time \cite{pachet2003continuator}. 



        %\subsection{MUSIC GENERATION}
        %\subsection{MELODY GENERATION}
        %\subsection{ACCOMPANIMENT GENERATION}
        %\subsection{INTERACTIVE GENERATIVE SYSTEMS}
        %\subsection{REAL-TIME ACCOMPANIMENT GENERATION}
    
    